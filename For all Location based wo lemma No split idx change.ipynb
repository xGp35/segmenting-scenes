{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from empath import Empath\n",
    "import requests\n",
    "import json\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "lexicon = Empath()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from stanza.server import CoreNLPClient\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lexicons(rb,lv,fp):\n",
    "    lexicon.create_category(\"religious_buildings\", [\"church\",\"mosque\", \"temple\"], model=\"fiction\", size = rb)\n",
    "    lexicon.create_category(\"loc_verbs\", [\"arrive\", \"visit\", \"travel\", \"return\"], model = \"fiction\", size= lv)\n",
    "    lexicon.create_category(\"fictional_places\", [\"place\",\"buildings\"], model =\"fiction\", size =fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"church\", \"temple\", \"chapel\", \"altar\", \"palace\", \"base\", \"manor\", \"castle\", \"cathedral\", \"graveyard\", \"cemetery\", \"chamber\", \"chambers\", \"gardens\", \"tower\", \"estate\", \"garden\", \"greenhouse\", \"tomb\", \"building\", \"valley\", \"Church\", \"shrine\", \"forge\", \"caravan\", \"alter\", \"city\", \"fortress\", \"keep\"]\n",
      "[\"return\", \"visit\", \"travel\", \"arrive\", \"depart\", \"leave\", \"relocate\", \"visiting\", \"accompany\", \"retire\", \"travel\", \"arriving\"]\n",
      "[\"buildings\", \"city\", \"houses\", \"other_buildings\", \"place\", \"tall_buildings\", \"huts\", \"structures\", \"cities\", \"other_houses\", \"ruins\", \"warehouses\", \"roads\", \"area\", \"vehicles\", \"skyscrapers\", \"tunnels\", \"mountains\", \"landscape\", \"areas\", \"pathways\", \"fences\", \"roofs\", \"towns\", \"building\", \"entire_place\", \"banks\", \"caves\", \"farms\", \"forests\", \"cottages\", \"countryside\", \"entire_city\", \"mansions\", \"factories\", \"tombs\", \"hills\", \"statues\", \"grid\", \"surrounding_area\", \"dome\", \"whole_city\", \"small_area\", \"whole_area\", \"alleyways\", \"cathedral\", \"homes\", \"trees\", \"plains\", \"greenery\", \"fields\", \"towers\", \"grounds\", \"residents\", \"entrances\", \"marketplace\", \"sidewalks\", \"borders\", \"places\", \"tourists\", \"perimeter\", \"gardens\", \"civilization\", \"entire_area\", \"doorways\", \"globe\", \"bridges\", \"factory\", \"terrain\", \"villages\", \"large_area\", \"few_places\", \"compound\", \"markets\", \"canyon\", \"shack\", \"farmland\", \"pillars\", \"Buildings\", \"palm_trees\", \"stone_walls\", \"pods\", \"rooftops\", \"structure\", \"alleys\", \"tower\", \"neighborhoods\", \"hillside\", \"cars\", \"cliffs\", \"columns\", \"small_village\", \"town\", \"wasteland\", \"warehouse\", \"vegetation\", \"windows\", \"white_house\", \"streets\", \"cavern\", \"open_space\", \"fortress\", \"tombstones\", \"Houses\", \"marsh\", \"whole_building\", \"hedges\", \"rubble\", \"sewers\", \"old_buildings\", \"boats\", \"turrets\", \"swamp\", \"small_houses\", \"landmarks\", \"grove\", \"farmhouse\", \"wreckage\", \"boulders\", \"tall_trees\", \"shrubbery\", \"apartments\", \"complex\", \"museum\", \"inhabitants\", \"underground\", \"enclosure\", \"domes\", \"harbor\", \"orchard\", \"ships\", \"acres\", \"wildlife\", \"underground\", \"brick_walls\", \"tents\", \"apartment_buildings\", \"jungle\", \"thick_trees\", \"many_places\", \"aircraft\", \"Trees\", \"graveyard\", \"outskirts\", \"wagons\", \"interior\", \"locals\", \"cluster\", \"plaza\", \"slopes\", \"slums\", \"city_streets\", \"walkways\", \"sights\", \"old_building\", \"dense_forest\", \"moat\", \"caravan\", \"shrubs\", \"shelters\", \"islands\", \"quarry\", \"small_building\", \"occupants\", \"trains\", \"constructed\", \"small_cabin\", \"walls\", \"mountain_range\", \"mountain\", \"valley\", \"masses\", \"corpses\", \"whole_place\", \"shacks\", \"civilisation\", \"maze\", \"different_parts\", \"different_areas\", \"rolling_hills\", \"castles\", \"many_trees\", \"torches\", \"land\", \"objects\", \"broken_windows\", \"fountains\", \"canals\", \"town_hall\", \"roof\", \"many_buildings\", \"castle\", \"wilderness\", \"entire_town\", \"caverns\", \"east\", \"large_city\", \"trucks\", \"small_shops\", \"exterior\", \"monument\", \"town_square\", \"village\", \"fenced\", \"lawns\", \"openings\", \"different_places\", \"balconies\", \"lighthouse\", \"bases\", \"neighbourhood\", \"facility\", \"small_cottage\", \"north\", \"castle_walls\", \"skyline\", \"other_areas\", \"tunnel\", \"glass_windows\", \"foothills\", \"shops\", \"planes\", \"outer_walls\", \"spaces\", \"docks\", \"woodland\", \"cottage\", \"small_island\", \"large_house\", \"routes\", \"lower_levels\", \"region\", \"boundary\", \"forest\", \"spaceship\", \"high_walls\", \"mirrors\", \"south\", \"businesses\", \"Manhattan\", \"distances\", \"glass_walls\", \"large_building\", \"other_trees\", \"neighborhood\", \"clusters\", \"coastline\", \"city_walls\", \"branching\", \"cabins\", \"planets\", \"port\", \"citadel\", \"settlement\", \"path\", \"mansion\", \"passageways\", \"debris\", \"other_building\", \"helicopters\", \"decks\", \"small_house\", \"dirt_roads\", \"large_field\", \"small_place\", \"expanse\", \"labyrinth\", \"churches\", \"graves\", \"craft\", \"footprints\", \"pines\", \"dead_bodies\", \"foliage\", \"air_vents\", \"ponds\", \"greenhouse\", \"restaurants\", \"sanctuary\", \"mainland\", \"layout\", \"gravestones\", \"bunker\", \"carriages\", \"lakes\", \"continent\", \"huge_wall\", \"abandoned_building\", \"stores\", \"caravans\", \"barns\", \"highways\", \"canopy\", \"railroad_tracks\", \"safe_zone\", \"ceilings\", \"bars\"]\n"
     ]
    }
   ],
   "source": [
    "create_lexicons(30,14,300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_sentence(sentence):\n",
    "    #tokenize the sentence and find the POS tag for each token\n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))  \n",
    "    #tuple of (token, wordnet_tag)\n",
    "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            #if there is no available tag, append the token as is\n",
    "            lemmatized_sentence.append(word)\n",
    "        else:        \n",
    "            #else use the tag to lemmatize the token\n",
    "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "    return \" \".join(lemmatized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the_story_of_the_merchant_son', 'the_thief_and_the_brahmins', 'the_monkey_and_the_crocodile', 'the_monkey_the_wedge', 'a_daring_plan', 'Buddha_remains_cool', 'moocha_raja', 'raman_horse_trainer', 'talkative_turtle', 'tenali_outwits_guards', 'tenali_the_detective']\n"
     ]
    }
   ],
   "source": [
    "story_names = []\n",
    "file = open(\"D:\\Jupyter\\BTP\\Panchatantra\\Storynames_new.txt\")\n",
    "file_story_names = file.readlines()\n",
    "for name in file_story_names:\n",
    "    story_names.append(name.strip('\\n'))\n",
    "file.close()\n",
    "print(story_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_story(Storyname):\n",
    "    file = open(\"D:\\Jupyter\\BTP\\Panchatantra\\\\\"+Storyname+'.txt', encoding = 'utf-8')\n",
    "    text = file.read()\n",
    "    text = unidecode(text)\n",
    "    file.close()\n",
    "    return text\n",
    "def annotate_story(text):\n",
    "    with CoreNLPClient(annotators = ['tokenize','ssplit'],\n",
    "        memory='5G', be_quiet=True, outputFormat = 'json', max_char_length=500000, timeout=36000000) as client:\n",
    "        annotated_story = client.annotate(text)\n",
    "    return annotated_story\n",
    "def open_and_annotate(Storyname):\n",
    "    text = open_story(Storyname)\n",
    "    annotated_story = annotate_story(text)\n",
    "    return text, annotated_story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_splitting = [[4, 14, 15, 16, 21, 22, 30, 46, 47, 52, 53],\n",
    "                [15, 16, 19, 20, 25, 26, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 45, 46],\n",
    "                [4, 5, 16, 17, 20, 26, 27, 28, 29, 39, 40, 44, 45, 46, 47, 53, 54, 55, 56, 57],\n",
    "                [],\n",
    "                [6, 15, 16, 26, 27, 28],\n",
    "                [11, 17, 18, 19, 20, 21, 22],\n",
    "                [],\n",
    "                [],\n",
    "                [10, 11, 12, 13, 14, 15, 16, 17, 20, 21, 22, 23, 24, 39],\n",
    "                [14, 15, 16, 17, 18, 30, 31, 32, 33, 34],\n",
    "                [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 24, 25, 34, 35, 36, 37, 38],\n",
    "               ]\n",
    "\n",
    "no_splitting_alternate = [[4, 5, 14, 15, 16, 17, 21, 22, 23, 30, 31, 46, 47, 48, 52, 53, 54],\n",
    "[15, 16, 17, 19, 20, 21, 25, 26, 27, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 45, 46, 47],\n",
    "[4, 5, 6, 16, 17, 18, 20, 21, 26, 27, 28, 29, 30, 39, 40, 41, 44, 45, 46, 47, 48, 53, 54, 55, 56, 57, 58],\n",
    "[]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def events_by_location_2(text, ann):\n",
    "    \"\"\"\n",
    "    Non-hierarchy model\n",
    "    \"\"\"\n",
    "    #This function finds sum of dictionary returned by lexicon.analyze i.e., it finds the presence of location words.\n",
    "    def sum_of_locs_dict(dictionary):\n",
    "        sum_ = 0\n",
    "        for key in dictionary.keys():\n",
    "            sum_ = sum_ + dictionary[key]\n",
    "        return sum_\n",
    "       \n",
    "    lexicon = Empath()   #Part of code used to bring Empath in\n",
    "    locations_dict = dict()     #Dictionary that holds\n",
    "    location = \"Unknown\"    #The variable place will hold latest location word.\n",
    "                            #It is initilized to \"unknown\" beacuse till now we haven't encountered any location word.\n",
    "    loaction_by_sentence = []\n",
    "    location_to_number = dict() # Convert location words to numbers for better representation\n",
    "    loc_num = 0 # Will be used to put location words as numbers in the location_to_number dict\n",
    "    total_sentences = 0\n",
    "    \n",
    "    #Take each sentence of the story one by one (ann.sentence returns individual sentences of the story as objects)\n",
    "    for i, sentence in enumerate(ann.sentence):\n",
    "        # Remove comma and fullstop beacuse lexicon.analyze cannot identify words if they are followd by a fullstop or comma.\n",
    "        # text[characterOffsetBegin:characterOffserEnd] is the actual sentence (as a string) of the sentence object returned\n",
    "        sentence_for_empath = text[sentence.characterOffsetBegin:sentence.characterOffsetEnd].replace(\", \",\" \").replace(\".\",\"\").replace(\"-\",\" \").replace(\"?\",\"\").replace(\"!\",\"\").replace(\":\",\" \")\n",
    "        #Lemmatize the words you encounter for better identification when being analysed by lexicon.analyze\n",
    "        #May be commented out because lexicon.create_category does not give good words when singular words are used\n",
    "        sentence_for_empath = lemmatize_sentence(sentence_for_empath) # Sentences are all lemmatized now\n",
    "        # Analyze the things\n",
    "        lexicon_locations_dict = lexicon.analyze(sentence_for_empath, categories=[\"religious_buildings\", \"loc_verbs\", \"fictional_places\"])\n",
    "        \n",
    "        s = sum_of_locs_dict(lexicon_locations_dict)\n",
    "        if s>0:\n",
    "            words = sentence_for_empath.split(\" \")\n",
    "            # Find if place is same as previous\n",
    "            for word in words:\n",
    "                # If the word is a location word\n",
    "                if sum_of_locs_dict(lexicon.analyze(word, categories=[\"religious_buildings\", \"loc_verbs\", \"fictional_places\"]))>0:\n",
    "                    #if new location word encountered is the same as the last location word encountered\n",
    "                    if word == location:\n",
    "                        break\n",
    "                    else:\n",
    "                        location = word\n",
    "                        if location in locations_dict:\n",
    "                            location+=\"1\"\n",
    "                        locations_dict[location]=[i-1]\n",
    "                        location_to_number[location] = loc_num\n",
    "                        loc_num += 1\n",
    "        else: \n",
    "            if location not in locations_dict:\n",
    "                locations_dict[location] = list()\n",
    "            locations_dict[location].append(i-1)\n",
    "        total_sentences = i\n",
    "    return locations_dict, location_to_number, total_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_event_list(locations_dict, no_split):\n",
    "    events= []\n",
    "    for location in locations_dict:\n",
    "        if locations_dict[location][0] not in no_split:\n",
    "            events.append(locations_dict[location][0])\n",
    "    events.sort()\n",
    "    events.append(total_sentences)\n",
    "    while events[0] == -1:\n",
    "        del events[0]\n",
    "    return set(events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-18 17:42:27 INFO: Writing properties to tmp file: corenlp_server-008177c06acc4b01.props\n",
      "2021-04-18 17:42:27 INFO: Starting server with command: java -Xmx5G -cp C:\\Users\\Giri\\stanza_corenlp\\* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 36000000 -threads 5 -maxCharLength 500000 -quiet True -serverProperties corenlp_server-008177c06acc4b01.props -annotators tokenize,ssplit -preload -outputFormat serialized\n",
      "2021-04-18 17:42:28 INFO: Writing properties to tmp file: corenlp_server-b850c13697504c5d.props\n",
      "2021-04-18 17:42:28 INFO: Starting server with command: java -Xmx5G -cp C:\\Users\\Giri\\stanza_corenlp\\* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 36000000 -threads 5 -maxCharLength 500000 -quiet True -serverProperties corenlp_server-b850c13697504c5d.props -annotators tokenize,ssplit -preload -outputFormat serialized\n",
      "2021-04-18 17:42:30 INFO: Writing properties to tmp file: corenlp_server-94de44e863224234.props\n",
      "2021-04-18 17:42:30 INFO: Starting server with command: java -Xmx5G -cp C:\\Users\\Giri\\stanza_corenlp\\* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 36000000 -threads 5 -maxCharLength 500000 -quiet True -serverProperties corenlp_server-94de44e863224234.props -annotators tokenize,ssplit -preload -outputFormat serialized\n",
      "2021-04-18 17:42:32 INFO: Writing properties to tmp file: corenlp_server-2cf432b2f5c44fe1.props\n",
      "2021-04-18 17:42:32 INFO: Starting server with command: java -Xmx5G -cp C:\\Users\\Giri\\stanza_corenlp\\* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 36000000 -threads 5 -maxCharLength 500000 -quiet True -serverProperties corenlp_server-2cf432b2f5c44fe1.props -annotators tokenize,ssplit -preload -outputFormat serialized\n",
      "2021-04-18 17:42:33 INFO: Writing properties to tmp file: corenlp_server-ac71de54a90d469b.props\n",
      "2021-04-18 17:42:33 INFO: Starting server with command: java -Xmx5G -cp C:\\Users\\Giri\\stanza_corenlp\\* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 36000000 -threads 5 -maxCharLength 500000 -quiet True -serverProperties corenlp_server-ac71de54a90d469b.props -annotators tokenize,ssplit -preload -outputFormat serialized\n",
      "2021-04-18 17:42:35 INFO: Writing properties to tmp file: corenlp_server-9e85f860830642e6.props\n",
      "2021-04-18 17:42:35 INFO: Starting server with command: java -Xmx5G -cp C:\\Users\\Giri\\stanza_corenlp\\* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 36000000 -threads 5 -maxCharLength 500000 -quiet True -serverProperties corenlp_server-9e85f860830642e6.props -annotators tokenize,ssplit -preload -outputFormat serialized\n",
      "2021-04-18 17:42:36 INFO: Writing properties to tmp file: corenlp_server-e3ded2e4d4834dbb.props\n",
      "2021-04-18 17:42:36 INFO: Starting server with command: java -Xmx5G -cp C:\\Users\\Giri\\stanza_corenlp\\* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 36000000 -threads 5 -maxCharLength 500000 -quiet True -serverProperties corenlp_server-e3ded2e4d4834dbb.props -annotators tokenize,ssplit -preload -outputFormat serialized\n",
      "2021-04-18 17:42:38 INFO: Writing properties to tmp file: corenlp_server-633387dc5f2143ef.props\n",
      "2021-04-18 17:42:38 INFO: Starting server with command: java -Xmx5G -cp C:\\Users\\Giri\\stanza_corenlp\\* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 36000000 -threads 5 -maxCharLength 500000 -quiet True -serverProperties corenlp_server-633387dc5f2143ef.props -annotators tokenize,ssplit -preload -outputFormat serialized\n",
      "2021-04-18 17:42:40 INFO: Writing properties to tmp file: corenlp_server-e2c6bb430089467b.props\n",
      "2021-04-18 17:42:40 INFO: Starting server with command: java -Xmx5G -cp C:\\Users\\Giri\\stanza_corenlp\\* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 36000000 -threads 5 -maxCharLength 500000 -quiet True -serverProperties corenlp_server-e2c6bb430089467b.props -annotators tokenize,ssplit -preload -outputFormat serialized\n",
      "2021-04-18 17:42:41 INFO: Writing properties to tmp file: corenlp_server-b56d0e8b93a14705.props\n",
      "2021-04-18 17:42:41 INFO: Starting server with command: java -Xmx5G -cp C:\\Users\\Giri\\stanza_corenlp\\* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 36000000 -threads 5 -maxCharLength 500000 -quiet True -serverProperties corenlp_server-b56d0e8b93a14705.props -annotators tokenize,ssplit -preload -outputFormat serialized\n",
      "2021-04-18 17:42:43 INFO: Writing properties to tmp file: corenlp_server-df94b98f9e614139.props\n",
      "2021-04-18 17:42:43 INFO: Starting server with command: java -Xmx5G -cp C:\\Users\\Giri\\stanza_corenlp\\* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 36000000 -threads 5 -maxCharLength 500000 -quiet True -serverProperties corenlp_server-df94b98f9e614139.props -annotators tokenize,ssplit -preload -outputFormat serialized\n"
     ]
    }
   ],
   "source": [
    "my_list_of_events = []\n",
    "for i,name in enumerate(story_names):\n",
    "    text , annotated_story = open_and_annotate(name)\n",
    "    locations_dict, location_number_map, total_sentences = events_by_location_2(text, annotated_story)\n",
    "    my_list_of_events.append(construct_event_list(locations_dict, no_splitting[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{5, 6, 7, 9, 26, 29, 32, 45, 50, 61, 64, 65},\n",
       " {1, 7, 17, 22, 24, 48},\n",
       " {8, 60},\n",
       " {0, 1, 3, 11},\n",
       " {8, 17, 33},\n",
       " {2, 32},\n",
       " {1, 16, 37},\n",
       " {18, 22},\n",
       " {6, 28, 32, 47, 49, 52, 56},\n",
       " {35, 37},\n",
       " {38}]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_list_of_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{5, 6, 7, 9, 26, 29, 32, 45, 50, 61, 64, 65},\n",
       " {1, 7, 17, 22, 24, 48},\n",
       " {0, 1, 3, 11},\n",
       " {8, 17, 33},\n",
       " {2, 32},\n",
       " {1, 16, 37},\n",
       " {18, 22},\n",
       " {6, 22, 28, 32, 47, 49, 52, 56},\n",
       " {35, 37},\n",
       " {38},\n",
       " {22, 31, 39, 43, 47, 51},\n",
       " {41, 63}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_list_of_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5, 6, 7, 9, 26, 29, 32, 45, 50, 61, 64, 65], [1, 7, 17, 22, 24, 48], [8, 60], [0, 1, 3, 11], [8, 17, 33], [2, 32], [1, 16, 37], [18, 22], [6, 28, 32, 47, 49, 52, 56], [35, 37], [38]]\n"
     ]
    }
   ],
   "source": [
    "my_list_of_events_2 = []\n",
    "for i,event in enumerate(my_list_of_events):    \n",
    "    x = sorted(list(event))\n",
    "    my_list_of_events_2.append(x)\n",
    "print(my_list_of_events_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-18 17:05:39 INFO: Writing properties to tmp file: corenlp_server-c992fac82b2e4ede.props\n",
      "2021-04-18 17:05:39 INFO: Starting server with command: java -Xmx5G -cp C:\\Users\\Giri\\stanza_corenlp\\* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 36000000 -threads 5 -maxCharLength 500000 -quiet True -serverProperties corenlp_server-c992fac82b2e4ede.props -annotators tokenize,ssplit -preload -outputFormat serialized\n"
     ]
    }
   ],
   "source": [
    "text , annotated_story = open_and_annotate('buddha_free_bird')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations_dict, location_number_map, total_sentences = events_by_location_2(text, annotated_story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{21}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "construct_event_list(locations_dict, [11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
